{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:02:41.752853Z",
     "iopub.status.busy": "2024-11-15T10:02:41.752102Z",
     "iopub.status.idle": "2024-11-15T10:02:41.758116Z",
     "shell.execute_reply": "2024-11-15T10:02:41.757160Z",
     "shell.execute_reply.started": "2024-11-15T10:02:41.752815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:02:41.760155Z",
     "iopub.status.busy": "2024-11-15T10:02:41.759787Z",
     "iopub.status.idle": "2024-11-15T10:03:15.783824Z",
     "shell.execute_reply": "2024-11-15T10:03:15.783049Z",
     "shell.execute_reply.started": "2024-11-15T10:02:41.760117Z"
    },
    "id": "3Kv-NtmzKkx4",
    "outputId": "84cd2817-26da-400a-815b-a1a34e220a27",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define a series of transformations for preprocessing and augmenting the training images\n",
    "train_transform = transforms.Compose([\n",
    "    # Resize and crop the image to 224x224 pixels\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    \n",
    "    # Horizontal flip to increase image diversity\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "    # Rotation of the image with a maximum of 15 degrees\n",
    "    transforms.RandomRotation(15),\n",
    "    \n",
    "    # Adjustment of brightness, contrast, saturation, and hue to simulate different lighting conditions\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "    \n",
    "    # Apply either Gaussian blur (7x7) or affine transformations (rotation, translation) with a 40% probability\n",
    "    transforms.RandomApply([transforms.GaussianBlur(7), transforms.RandomAffine(degrees=15)], p=0.4),\n",
    "    \n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the pixel values using the ImageNet mean and standard deviation (commonly used for pre-trained models)\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# The transformations for validation and test images are simpler\n",
    "val_test_transform = transforms.Compose([\n",
    "    # Resize the image to 256 pixels along the longest side\n",
    "    transforms.Resize(256),\n",
    "    \n",
    "    # Perform a central crop to get a 224x224 image\n",
    "    transforms.CenterCrop(224),\n",
    "    \n",
    "    # Convert the image to a tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize the pixel values using the same ImageNet mean and standard deviation\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the training dataset with the applied transformations\n",
    "train_data = datasets.ImageFolder(\"/kaggle/input/deepfake-and-real-images/Dataset/Train\", transform=train_transform)\n",
    "\n",
    "# Load the validation dataset with the applied validation transformations\n",
    "val_data = datasets.ImageFolder(\"/kaggle/input/deepfake-and-real-images/Dataset/Validation\", transform=val_test_transform)\n",
    "\n",
    "# Load the test dataset with the applied test transformations\n",
    "test_data = datasets.ImageFolder(\"/kaggle/input/deepfake-and-real-images/Dataset/Test\", transform=val_test_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:15.785237Z",
     "iopub.status.busy": "2024-11-15T10:03:15.784925Z",
     "iopub.status.idle": "2024-11-15T10:03:18.603209Z",
     "shell.execute_reply": "2024-11-15T10:03:18.602208Z",
     "shell.execute_reply.started": "2024-11-15T10:03:15.785204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n",
      "100%|██████████| 233M/233M [00:01<00:00, 191MB/s]  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.models.segmentation as segmentation\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Defines a model wrapper for the pre-trained DeepLabV3 segmentation model\n",
    "class DeepLabV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLabV3, self).__init__()\n",
    "        self.model = segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "    \n",
    "# Performs segmentation on a single image, generating a mask\n",
    "def segment_image(model, image, device):\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    mask = output.argmax(1).squeeze().cpu().numpy()\n",
    "    return mask\n",
    "\n",
    "# Applies segmentation to an image and returns the masked image\n",
    "class SegmentationTransform:\n",
    "    def __init__(self, segmentation_model, device):\n",
    "        self.segmentation_model = segmentation_model\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_tensor = TF.to_tensor(img).to(self.device)\n",
    "        mask = segment_image(self.segmentation_model, img_tensor, self.device)\n",
    "        mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        img = TF.to_tensor(img).to(self.device)\n",
    "        return img * mask_tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segmentation_model = DeepLabV3().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism and Adaptive Weighting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:18.605727Z",
     "iopub.status.busy": "2024-11-15T10:03:18.605410Z",
     "iopub.status.idle": "2024-11-15T10:03:18.621208Z",
     "shell.execute_reply": "2024-11-15T10:03:18.619979Z",
     "shell.execute_reply.started": "2024-11-15T10:03:18.605694Z"
    },
    "id": "wsfujRxOKk0y",
    "outputId": "7f3a6345-9297-4db6-ab53-7c8563936e8f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Checks if a GPU (CUDA-enabled) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Attention mechanism\n",
    "class GLCSAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(GLCSAttention, self).__init__()\n",
    "        # 1x1 convolutions that help to capture local and global channel attention information.\n",
    "        self.conv1x1_local = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.conv1x1_global_1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.conv1x1_global_2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        # Capture spatial attention at multiple scales.\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.conv5x5 = nn.Conv2d(in_channels, in_channels, kernel_size=5, padding=2)\n",
    "        self.conv7x7 = nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3)\n",
    "        # Activation function to normalize the attention maps between 0 and 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "# Defines the forward pass for the attention mechanism\n",
    "    def forward(self, x):\n",
    "        # Compute local channel attention and apply to input\n",
    "        local_channel_attention = self.sigmoid(self.conv1x1_local(F.adaptive_avg_pool2d(x, (1, 1))))\n",
    "        local_channel_attention = x * local_channel_attention\n",
    "        # Compute global channel attention and apply to input\n",
    "        global_channel_attention = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        global_channel_attention = self.sigmoid(self.conv1x1_global_1(global_channel_attention) * self.conv1x1_global_2(global_channel_attention))\n",
    "        global_channel_attention = x * global_channel_attention\n",
    "        # Compute local spatial attention using multiple kernel sizes and apply to input\n",
    "        local_spatial_attention = self.conv1x1_local(x)\n",
    "        local_spatial_attention = self.conv3x3(local_spatial_attention) + self.conv5x5(local_spatial_attention) + self.conv7x7(local_spatial_attention)\n",
    "        local_spatial_attention = self.sigmoid(local_spatial_attention)\n",
    "        local_spatial_attention = x * local_spatial_attention\n",
    "        # Compute global spatial attention and apply to input\n",
    "        global_spatial_attention = self.conv1x1_local(x)\n",
    "        global_spatial_attention = self.sigmoid(global_spatial_attention * self.conv1x1_local(global_spatial_attention))\n",
    "        global_spatial_attention = x * global_spatial_attention\n",
    "        # Combine local and global attention (both channel and spatial) to get the final attention map\n",
    "        final_attention = (0.5 * local_channel_attention + 0.5 * global_channel_attention) * (0.5 * local_spatial_attention + 0.5 * global_spatial_attention)\n",
    "        return final_attention\n",
    "\n",
    "\n",
    "class AdaptivelyWeightedMultiScaleAttention(nn.Module):\n",
    "    def __init__(self, scales):\n",
    "        super(AdaptivelyWeightedMultiScaleAttention, self).__init__()\n",
    "        # Define GLCSAttention modules for each scale and the learnable weights\n",
    "        self.scales = scales\n",
    "        self.glcs_attention = nn.ModuleList([GLCSAttention(scale) for scale in scales])\n",
    "        self.weights = nn.Parameter(torch.ones(len(scales)) * 0.25)\n",
    "\n",
    "    def forward(self, features):\n",
    "        weighted_features = []\n",
    "        for i, feature in enumerate(features):\n",
    "            attention = self.glcs_attention[i](feature)\n",
    "            weight = self.weights[i]\n",
    "            weighted_feature = F.adaptive_max_pool2d(attention, (1, 1)) * weight\n",
    "            weighted_features.append(weighted_feature)\n",
    "        return torch.cat(weighted_features, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FaceNeSt Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:18.622874Z",
     "iopub.status.busy": "2024-11-15T10:03:18.622543Z",
     "iopub.status.idle": "2024-11-15T10:03:19.767799Z",
     "shell.execute_reply": "2024-11-15T10:03:19.767004Z",
     "shell.execute_reply.started": "2024-11-15T10:03:18.622841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FaceNeSt(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FaceNeSt, self).__init__()\n",
    "        self.initial_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.resnet_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 256, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "        ])\n",
    "        self.adaptive_attention = AdaptivelyWeightedMultiScaleAttention([64, 256, 512, 1024])\n",
    "        self.conv1x1 = nn.Conv2d(sum([64, 256, 512, 1024]), 512, kernel_size=1)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layers(x)\n",
    "        features = [x]\n",
    "        for block in self.resnet_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "        x = self.adaptive_attention(features)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = FaceNeSt(num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling and DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:19.769274Z",
     "iopub.status.busy": "2024-11-15T10:03:19.768949Z",
     "iopub.status.idle": "2024-11-15T10:03:19.785823Z",
     "shell.execute_reply": "2024-11-15T10:03:19.784859Z",
     "shell.execute_reply.started": "2024-11-15T10:03:19.769241Z"
    },
    "id": "HJ3DfwqIRcbW",
    "outputId": "94deab14-6bae-4c0e-8a65-747e1e2a3fcd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset_proportion = 0.1\n",
    "\n",
    "train_indices = np.random.choice(len(train_data), int(len(train_data) * subset_proportion), replace=False)\n",
    "val_indices = np.random.choice(len(val_data), int(len(val_data) * subset_proportion), replace=False)\n",
    "test_indices = np.random.choice(len(test_data), int(len(test_data) * subset_proportion), replace=False)\n",
    "\n",
    "train_subset = Subset(train_data, train_indices)\n",
    "val_subset = Subset(val_data, val_indices)\n",
    "test_subset = Subset(test_data, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup and Optimization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:19.787358Z",
     "iopub.status.busy": "2024-11-15T10:03:19.787046Z",
     "iopub.status.idle": "2024-11-15T10:03:19.793999Z",
     "shell.execute_reply": "2024-11-15T10:03:19.793059Z",
     "shell.execute_reply.started": "2024-11-15T10:03:19.787325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "scaler = GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation with Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:19.795893Z",
     "iopub.status.busy": "2024-11-15T10:03:19.795289Z",
     "iopub.status.idle": "2024-11-15T10:03:19.806707Z",
     "shell.execute_reply": "2024-11-15T10:03:19.805760Z",
     "shell.execute_reply.started": "2024-11-15T10:03:19.795851Z"
    },
    "id": "btYcXp6KRce3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_with_accumulation(model, loader, criterion, optimizer, device, accumulation_steps):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        loss = loss / accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item() * inputs.size(0) * accumulation_steps\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters and Metrics Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:19.810239Z",
     "iopub.status.busy": "2024-11-15T10:03:19.809855Z",
     "iopub.status.idle": "2024-11-15T10:03:19.817283Z",
     "shell.execute_reply": "2024-11-15T10:03:19.816147Z",
     "shell.execute_reply.started": "2024-11-15T10:03:19.810208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "best_val_acc = 0.0\n",
    "accumulation_steps = 8\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T10:03:19.819157Z",
     "iopub.status.busy": "2024-11-15T10:03:19.818811Z",
     "iopub.status.idle": "2024-11-15T13:24:15.858824Z",
     "shell.execute_reply": "2024-11-15T13:24:15.856321Z",
     "shell.execute_reply.started": "2024-11-15T10:03:19.819123Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.8054, Train Accuracy: 0.5179, Val Loss: 0.6926, Val Accuracy: 0.5335\n",
      "Time taken in Epoch 1: 13 minutes and 15 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 0.6922, Train Accuracy: 0.5388, Val Loss: 0.6866, Val Accuracy: 0.5596\n",
      "Time taken in Epoch 2: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 0.6900, Train Accuracy: 0.5397, Val Loss: 0.6823, Val Accuracy: 0.5540\n",
      "Time taken in Epoch 3: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 0.6820, Train Accuracy: 0.5591, Val Loss: 0.6660, Val Accuracy: 0.5994\n",
      "Time taken in Epoch 4: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 0.6923, Train Accuracy: 0.5464, Val Loss: 0.6739, Val Accuracy: 0.5619\n",
      "Time taken in Epoch 5: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 0.6752, Train Accuracy: 0.5813, Val Loss: 0.6649, Val Accuracy: 0.5987\n",
      "Time taken in Epoch 6: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 0.6862, Train Accuracy: 0.5625, Val Loss: 0.7020, Val Accuracy: 0.5096\n",
      "Time taken in Epoch 7: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 0.6772, Train Accuracy: 0.5736, Val Loss: 0.6565, Val Accuracy: 0.6175\n",
      "Time taken in Epoch 8: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 0.6775, Train Accuracy: 0.5881, Val Loss: 0.6524, Val Accuracy: 0.6284\n",
      "Time taken in Epoch 9: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 0.6705, Train Accuracy: 0.5851, Val Loss: 0.6494, Val Accuracy: 0.6225\n",
      "Time taken in Epoch 10: 13 minutes and 24 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 0.6685, Train Accuracy: 0.5926, Val Loss: 0.6301, Val Accuracy: 0.6547\n",
      "Time taken in Epoch 11: 13 minutes and 24 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 0.6610, Train Accuracy: 0.6069, Val Loss: 0.6411, Val Accuracy: 0.6385\n",
      "Time taken in Epoch 12: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 0.6514, Train Accuracy: 0.6139, Val Loss: 0.6175, Val Accuracy: 0.6555\n",
      "Time taken in Epoch 13: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 0.6496, Train Accuracy: 0.6170, Val Loss: 0.6164, Val Accuracy: 0.6626\n",
      "Time taken in Epoch 14: 13 minutes and 23 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 0.6457, Train Accuracy: 0.6238, Val Loss: 0.6202, Val Accuracy: 0.6550\n",
      "Time taken in Epoch 15: 13 minutes and 24 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "best_val_acc = 0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop with progress bar\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0:  # Gradient accumulation\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy for training set (if classification)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Validation loop with progress bar\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():  \n",
    "        for inputs, targets in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy for validation set (if classification)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Scheduler step based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    total_time = int(end_time - start_time)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\\n'\n",
    "          f'Time taken in Epoch {epoch+1}: {total_time//60:.0f} minutes and {total_time%60} seconds', end=\"\\n\\n\")\n",
    "    \n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "#         early_stopping_counter = 0\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "\n",
    "#     if early_stopping_counter >= patience:\n",
    "#         print(\"Early stopping\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-15T13:24:15.860796Z",
     "iopub.status.busy": "2024-11-15T13:24:15.860459Z",
     "iopub.status.idle": "2024-11-15T13:24:15.866289Z",
     "shell.execute_reply": "2024-11-15T13:24:15.865313Z",
     "shell.execute_reply.started": "2024-11-15T13:24:15.860759Z"
    },
    "id": "1yEwE3vrRdQB",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     train_loss = train_with_accumulation(model, train_loader, criterion, optimizer, device, accumulation_steps)\n",
    "#     val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "#     scheduler.step(val_loss)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accuracies.append(val_acc)\n",
    "    \n",
    "#     total_time = int(end_time - start_time)\n",
    "\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\\nTime taken in Epoch {epoch + 1}: {total_time/60:.0f} minutes and {total_time%60} seconds', end=\"\\n\\n\")\n",
    "    \n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "#         early_stopping_counter = 0\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "\n",
    "#     if early_stopping_counter >= patience:\n",
    "#         print(\"Early stopping\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Training and Validation Loss/Accuracy Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:29:28.767597Z",
     "iopub.status.busy": "2024-11-15T13:29:28.766815Z",
     "iopub.status.idle": "2024-11-15T13:29:28.811148Z",
     "shell.execute_reply": "2024-11-15T13:29:28.810307Z",
     "shell.execute_reply.started": "2024-11-15T13:29:28.767556Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"e63ba10f-8ce8-47b3-a115-dc52e35b87c7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e63ba10f-8ce8-47b3-a115-dc52e35b87c7\")) {                    Plotly.newPlot(                        \"e63ba10f-8ce8-47b3-a115-dc52e35b87c7\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"y\":[0.8053986751325598,0.6922191763849563,0.6899674786552447,0.6820255719363417,0.6922919275281636,0.6752278061762248,0.6862496192052484,0.6771767117661428,0.6775393378516855,0.6705298493169758,0.6685395069318275,0.6609675591666949,0.651422139716475,0.6496172393565853,0.6457177606075322],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Validation Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"y\":[0.6925605972928386,0.6865584571515361,0.6822872892502816,0.6660292105328652,0.6739317635374684,0.6648663061280404,0.7019544772563442,0.656539082046478,0.6524079836183979,0.6494255575441545,0.630110954084704,0.6410616564173852,0.6175189323482975,0.616372876590298,0.6201570048447578],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Validation Accuracy\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"y\":[0.5334855403348554,0.5596144089294774,0.5540334855403348,0.5994419076610857,0.5618975139523085,0.5986808726534754,0.5096397767630645,0.617453069507864,0.6283612379502791,0.6225266362252664,0.6547437848807712,0.6385083713850838,0.6555048198883815,0.6626078132927448,0.6549974632166413],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Epochs\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Loss over Epochs\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Accuracy over Epochs\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Training and Validation Metrics Over Epochs\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e63ba10f-8ce8-47b3-a115-dc52e35b87c7');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss over Epochs\", \"Accuracy over Epochs\"))\n",
    "\n",
    "# First subplot - Train and Validation Loss\n",
    "fig.add_trace(go.Scatter(x=list(range(len(train_losses))), y=train_losses, mode='lines', name='Train Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=list(range(len(val_losses))), y=val_losses, mode='lines', name='Validation Loss'), row=1, col=1)\n",
    "\n",
    "# Second subplot - Validation Accuracy\n",
    "fig.add_trace(go.Scatter(x=list(range(len(val_accuracies))), y=val_accuracies, mode='lines', name='Validation Accuracy'), row=1, col=2)\n",
    "\n",
    "# Update layout and axis titles\n",
    "fig.update_layout(title_text=\"Training and Validation Metrics Over Epochs\")\n",
    "fig.update_xaxes(title_text=\"Epochs\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Epochs\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:25:04.314079Z",
     "iopub.status.busy": "2024-11-15T13:25:04.313345Z",
     "iopub.status.idle": "2024-11-15T13:25:12.240560Z",
     "shell.execute_reply": "2024-11-15T13:25:12.239352Z",
     "shell.execute_reply.started": "2024-11-15T13:25:04.314041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3238351487.py:1: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6209, Test Accuracy: 0.6716\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1909705,
     "sourceId": 3134515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
